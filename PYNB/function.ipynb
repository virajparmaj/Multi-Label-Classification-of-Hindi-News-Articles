{
  "metadata": {
    "language_info": {
      "name": ""
    },
    "kernelspec": {
      "name": "",
      "display_name": ""
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nfrom nltk.tokenize import word_tokenize\nimport math\nimport csv\n\n# define punctuations list\npunctuations = ['nn','n', 'ред','/', '`', '+', '\"', '?', '_(', '$', '@', '[', '_', \"'\", '!', ',', ':', '^', '|', ']', '=', '%', '&', '.', ')', '(', '#', '*', '', ';', '-', '}','|','\"','\\\\', \"''\", '``']\n\n\n\n# Loadstopwords from \"stopwords.csv\"\ndef load_stopwords(csv_file):\n    stopwords = []\n    with open(csv_file) as file:\n        reader = csv.reader(file)\n        for row in reader:\n            for word in row:\n                if len(word.strip()) != 0:\n                    stopwords.append(word.strip())\n    return stopwords\n\n\n\n# remove stopwords from 'tokens' list\ndef stopword(tokens,stopwords):\n    t = 0\n    while (t < len(tokens)):\n        if tokens[t].strip() in stopwords or tokens[t].strip('ред ') in stopwords or tokens[t].strip().isnumeric():\n            item = tokens[t]\n            c = tokens.count(item)\n            for n in range(c):\n                tokens.remove(item)\n        else:\n            tokens[t] = tokens[t].strip('ред ')\n        t += 1\n\n\n\n# calculate tf\n# tf(w) = (frequency of word 'w' in an article/ total words in the article) averaged over articles in a class\ndef calculate_tf(dataset,stopwords):\n    cols = dataset.columns.ravel()\n    labels_tf = []\n    for i in range(3,17):\n        label_articles = dataset[dataset[cols[i]] == 1]['desc']\n        label = []\n        for article in label_articles:\n            # tokenize article\n            tokens = word_tokenize(article)\n\n            # remove stopwords from 'tokens' list\n            stopword(tokens,stopwords)\n\n            # convert list of tokens to dataframe\n            tokens_df = pd.DataFrame(data=tokens, columns=['token'])\n\n            # calculate count of each token(divided by article length) and stored in a series frame\n            sf = tokens_df.value_counts() / len(tokens_df)\n\n            # convert series frame to dataframe\n            df = sf.to_frame().reset_index()\n            df = df.rename(columns={0: 'tf'})\n\n            # append dataframe containing token count to the list 'label'\n            label.append(df)\n\n        # calculate 'tf' for a label by summing over each articles for a label and taking average(divided by no of articles)\n        df1 = pd.concat(label, ignore_index=True).groupby(['token'], as_index=False).sum()\n        df1['tf'] = df1['tf'] / len(label)\n\n        # append the dataframe containing 'tf' value for a label to a list\n        labels_tf.append(df1)\n    return labels_tf\n\n\n\n# calculate 'idf' for each word in the document\n# idf(w) = log(total words in corpus/ frequency of word 'w' in the corpus)\n# used formula: idf(w) = log(total articles in corpus/ frequency of articles with word 'w' in the corpus)\ndef calculate_idf(dataset, stopwords):\n    corpus_articles = dataset['desc']\n    corpus = []\n    word_count = 0\n    for article in corpus_articles:\n        # tokenize article\n        tokens = list(set(word_tokenize(article))) # to get unique tokens\n        word_count += len(tokens)\n\n        # remove stopwords from 'tokens' list\n        stopword(tokens, stopwords)\n\n        # convert list of tokens to dataframe\n        tokens_df = pd.DataFrame(data=tokens, columns=['token'])\n\n        # add a count column and set value as '1'\n        tokens_df['count'] = 1\n\n        # append dataframe containing token count to the list 'corpus'\n        corpus.append(tokens_df)\n\n    # calculate idf for each word in the document\n    idf_df = pd.concat(corpus, ignore_index=True).groupby(['token'], as_index=False).sum()\n\n    # changing column name to 'idf' from 'count'\n    idf = idf_df.copy()\n    idf['count'] = np.log((len(corpus_articles)-idf['count']+0.5)/ (idf['count']+0.5))  #apply idf formula\n    idf = idf.rename(columns={'count': 'idf'})\n    return idf\n\n\n\n# calulate (tf * idf) for each token of each label\ndef calculate_tf_idf(labels_tf, idf):\n    tf_idf = []\n    for label_tf in labels_tf:\n        label_tf_itf = pd.merge(label_tf, idf, on='token')\n        label_tf_itf['tf-idf'] = label_tf_itf['tf'] * label_tf_itf['idf']\n        label_tf_itf = label_tf_itf.sort_values(by=['tf-idf'], ascending=False)\n        tf_idf.append(label_tf_itf)\n    return tf_idf\n\n\n\n# store top 100 tokens for each label in different excel sheets in a output file (arranged in descending order of tf-itf score)\ndef store_features(tf_idf, cols, output_file):\n    i = 3\n    frames = {}\n    for df in tf_idf:\n        sheet = cols[i]\n        frames[sheet] = df.head(100)\n        i += 1\n\n    with pd.ExcelWriter(output_file) as writer:\n        for frame in frames:\n            frames[frame].to_excel(writer, sheet_name=frame)\n\n\n\n# calculate tf-idf score for an article\ndef score(article, label_features, cols, stopwords):\n    tokens = word_tokenize(article)\n    stopword(tokens, stopwords)\n    scores = []\n    for i in range(3, 17):\n        sheet = cols[i]\n        features = label_features[i-3]\n        dic = {}\n        top_tokens = features['token']\n        for token in top_tokens:\n            dic[token] = list(features[features['token']==token]['tf-idf'])[0]\n        score = 0\n        for token in tokens:\n            if token in dic:\n                score += dic[token]\n        scores.append(score)\n    return scores",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}